You are training a classification model with logistic

regression. Which of the following statements are true? Check

all that apply.

Introducing regularization to the model always results in equal or better performance on examples not in the training set.

Introducing regularization to the model always results in equal or better performance on the training set.

Adding many new features to the model makes it more likely to overfit the training set.

Adding a new feature to the model always results in equal or better performance on examples not in the training set.
Question 21

Question 2
Suppose you ran logistic regression twice, once with λ=0, and once with λ=1. One of the times, you got

parameters θ=[26.2965.41], and the other time you got

θ=[2.751.32]. However, you forgot which value of

λ corresponds to which value of θ. Which one do you

think corresponds to λ=1?

θ=[26.2965.41]

θ=[2.751.32]
Question 31


Question 3
Which of the following statements about regularization are

true? Check all that apply.

Using a very large value of λ cannot hurt the performance of your hypothesis; the only reason we do not set λ to be too large is to avoid numerical problems.

Because regularization causes J(θ) to no longer be convex, gradient descent may not always converge to the global minimum (when λ>0, and when using an appropriate learning rate α).

Because logistic regression outputs values 0≤hθ(x)≤1, its range of output values can only be "shrunk" slightly by regularization anyway, so regularization is generally not helpful for it.

Using too large a value of λ can cause your hypothesis to underfit the data.

Analysis: 
for question 3:


